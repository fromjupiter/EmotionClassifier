\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Face Emotion Classification Using Single-Layer Neural Networks}


\author{
Kexiang Feng \\
Department of Computer Science and Engineering\\
University of California, San Diego\\
\texttt{fkxcole@gmail.com} \\
\And
Xupeng Yu \\
Department of Electrical and Computer Engineering \\
University of California, San Diego \\
\texttt{maiyuxiaoge@gmail.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
This paper summarizes steps to build a face emotion neural network without using any machine learning libraries(Sklearn, Pytorch, etc.). We start with a logistic regression layer to solve binary classification problem, then we use a softmax layer to deal with more general multi-label classification. All models are evaluated on the CK+ dataset using 10-fold cross validation. Our logistic regressor achieves an average accuracy of $ 100\% $ in Happy vs Angry classification and our softmax regressor achieves an average accuracy of 78\% in 6-way classification.
\end{abstract}

\section{Introduction}
In what section we describe what problems to get the scores on what part of the assignment.

In section 2 we briefly introduce our data preprocessing and data split, namely Principal Component Analysis and K-Fold. Section 3 and 4 each elaborates our work in the logistic regressor and softmax regressor. Following that we share our findings in Section 5. Lastly we briefly introduce individual contributions in section 6.

\section{Dataset and Task Description}

what is CK+? what is aligned vs resized? What preprocess (PCA) do we need?
DATA\_SAMPLER

\subsection {Principal Component Analysis}
\subsection {K-Fold Cross Validation}


\section{LOGISTIC REGRESSION}
Basic ideas of LR. 
\subsection {Evaluation on Happiness vs Anger using the resized dataset}
\subsection {Evaluation on Happiness vs Anger using the aligned dataset}
\subsection {Evaluation on Fear vs Surprise using the aligned dataset}


\section{SOFTMAX REGRESSION}
Softmax is the generalization of logistic regression for multiple classes. Given an input $x^n$, softmax regression will output a vector $y^n$, where each element, $y_k^n$ represents the probability that $x^n$ is in class k.
\subsection {Model Evaluation}
We performed an 10-fold cross validation on pictures of six emotions to check our model's performance. In each fold, best model parameters is selected so that holdout dataset reaches lowest loss. Then we use the model parameters to record the accuracy on test dataset. Final model performance is the average recorded accuracy of ten folds. \\
Below figure shows the training process and the result in confusion matrix form.
\subsection {Weight Visualization}
To show that our model has learned meaningful patterns, we can visualize the model parameters by doing inner product with our PCA components. Below pictures show the result of visualized weights. 
\subsection {Batch vs Stochastic Gradient Descent}
In normal batch gradien descent(GD), weights are updated once per epoch. To increase the converging speed over every epoch, stochastic gradient descent(SGD) is introduced.\\
The figure below compares the converging speed between GD and SGD. We can see that SGD is much faster in terms of minimizing the loss in fewer epochs. The reason is that in each SGD epoch, model parameters are updated N times(N is the training set size) instead of one time.
\subsection {Class weights to handle imbalanced dataset}
The above experiments have been performed on a balanced dataset. In order to handle imbalanced dataset, we modify our model by adding class weights. The basic idea is that we want to give a penalty to the major classes and a bonus to the rare classes. Class weights are defined as $c_k = C*N/N_k$ where C is class number, N is sample number and $N_k$ is sample number with class k. \\
Given this class weight definition, we define weighted cross entropy $E = -\sum_{n}\sum_{k=1}^{C} c_k t_k^n ln y_k^n $ as our loss function. The target of our new model training process is to minimize the weighted cross entropy. \\
Below figure compares the modified model with the original model using the whole CK+ dataset.

\section {CONCLUSION}
\section{INDIVIDUAL CONTRIBUTIONS}
Our group consists of two members: \textit{Kexiang Feng} and \textit{Xupeng Yu}. \\
\textit{Kexiang Feng} builds the PCA, K-Fold modules and Softmax regressor. He also wrote the scripts to visualize the softmax weights and PCA components. \\
\textit{Xupeng Yu} builds the Logistic regressor. 

\section {References}
\small{
[1]
}

\end{document}
